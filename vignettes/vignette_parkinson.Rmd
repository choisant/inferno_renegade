---
title: "Example of use: inferences for Parkinson's Disease"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Example of use: inferences for Parkinson's Disease}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Purpose

Here is a toy example showing some kinds of results that can be obtained with Bayesian nonparametric exchangeable inference. The example involves inferences about some investigations on Parkinson's Disease described in the paper by [Brakedal & al. (2022)](https://doi.org/10.1016/j.cmet.2022.02.001), which generously and professionally shares its [data](https://www.cell.com/cms/10.1016/j.cmet.2022.02.001/attachment/82e51142-50b8-4f84-a3e0-7c08d641a47b/mmc2.xlsx).

The investigations in the cited paper involve 30 subjects, for each of which various data are collected. Fifteen of these subjects belong to a control group, the other 15 to the treatment group, denoted `NR`. In this toy example we do not follow or reproduce the paper's investigations, nor do we use all the variates observed. We limit ourselves to 7 variates for each subject. Here is a list with a brief description of each:

#### Treatment group

*Binary* variate with values `Placebo` and `NR`.

#### Sex

*Binary* variate with values `Female` and `Male`.

#### Age

*Integer* variate.

#### Anamnestic loss of smell

*Binary* variate with values `No` and `Yes`.

#### History of REM sleep behaviour disorder

*Binary* variate with values `No` and `Yes`.

#### MDS-UPDRS-III score at the first subject visit

The [MDS-UPDRS-III](https://www.movementdisorders.org/MDS-Files1/PDFs/Rating-Scales/MDS-UPDRS_English_FINAL.pdf) (Movement Disorder Society's Unified Parkinson's Disease Rating Scale) is one of a set of *ordinal* scales to assess a subject's degree of disability and monitor it over time. The scale III ranges from `0` (no disability) to `132` (highest degree of disability). Given that operations such as sum or average are performed on these scales in the literature, the scales should be considered on an *interval* kind rather than simply ordinal.

#### Difference in MDS-UPDRS-III scores between second and first visit

*Ordinal* variate, the difference of the previous one between the subject's second and first visit. Its range is therefore from `-132` to `+132`, *negative* values representing an *improvement*.

\

We store the name of the datafile:example:
```{r}
datafile <- 'toydata.csv'
```
Here are the values for two subjects:
```{r echo = FALSE}
knitr::kable(read.csv(datafile, na.strings='')[1:2,])
```

### "Natural" vs controlled or "biased" variates

For the purpose of drawing scientific inferences, it is extremely important to be mindful of which variates in the dataset have "natural" statistics, and which instead have statistics controlled by the observer -- and are therefore "biased". This distinction is important because the dataset statistics of the controlled variates may then *not* be representative of the naturally-occurring ones, and therefore cannot be used for inference purposes.

In the present dataset, for instance, the numbers of `Placebo` and `NR` subjects were decided by the investigators to be in a 50%/50% proportion. Obviously this is not a naturally occurring proportion; in fact, it does not even make sense of speaking of a naturally occurring statistics for this `TreatmentGroup` variate.

For the `Sex` variate, the dataset has a proportion of around 17% `Female` and 83% `Male` subjects. This proportion also partly stems from the selection procedure of the investigators, so we cannot infer that roughly 16% of Parkinson patients in the national population are females. It is also possible that the proportions in the `Age` variate may depend on the selection procedure.

On the other hand, the investigators did *not* choose the statistics of the two `MDS.UPDRS` variates, within the groups having particular values of the other variates. This *conditional* or *subpopulation* statistics can be taken as representatives of naturally-occurring ones (see [Lindley & Novick (1980)](https://doi.org/10.1214/aos/1176345331) for a brilliant discussion of this point).

*(add short explanation of exchangeability)*

## Metadata

Let's load the package:
```{r setup}
library(inferno)
```


In order to draw inferences, we need metadata information, that is, information about the variates that is *not* present in the data. Such information mainly regard the kind and domain of each variate.

As an example of why metadata is necessary, suppose you are given the data `3`, `7`, `9`. Can you say whether the value `5` could be observed in a new instance? and what about `4.7`, or `-10`? Knowing whether these values are at all possible, or not, greatly affects the precision of the inferences we draw. Obviously a researcher must know such basic information about the data being collected -- otherwise they wouldn't know what they're doing.

The software requires specification of metadata either as a `data.frame` object or as a `.csv` file. To help you prepare this file, the software offers the function `buildmetadata()`, which writes a template that you thereafter modify. The template is filled with **tentative** values; these are wild guesses from the data, so they are often wrong. The function explains the heuristic it is using in order to pre-fill the fields of the metadata file, and gives warnings about guessed values that are especially dubious. You should examine and correct all fields:
```{r}
buildmetadata(data = datafile, file = 'temp_metadata')
```

We can open the temporary metadata file `temp_metadata.csv` and edit it with any software of our choice. Let's save the corrected version as `meta_toydata.csv`:
```{r}
metadatafile <- 'meta_toydata.csv'
```
The metadata are as follows; `NA` indicate empty fields:
```{r echo = FALSE}
knitr::kable(read.csv(metadatafile, na.strings = ''))
```
For a description of the metadata fields, see the help for `metadata` with `?inferno::metadata`.

*(More to be written here!)*


## Learning

Our inferences will be based on the data already observed, and will mainly take the form of *probabilities* about the unkonwn values various quantities $Y$, given the observed values of other quantities $X$:
$$
\mathrm{Pr}(Y = y \:\vert\: X = x, \mathrm{data})
$$
As the notation above indicates, *these probabilities also depend on the $\mathrm{data}$ already observed*. They are usually called "posterior probabilities".

We need to prepare the software to perform calculations of posterior probabilities given the observed data. In machine learning an analogous process is called "learning". For this reason the function that achieves this is called `learn()`. It requires at least three arguments:

- `data`, which can be given as a path to the `csv` file containing the data
- `metadata`, which can also be given as a path to the metadata file
- `outputdir`: the name of the directory where the output should be saved.

It may be useful to specify two more arguments:

- `seed`: the seed for the random-number generator, to ensure reproducibility
- `parallel`: the number of CPUs to use for the computation

Alternatively you can set the seed with `set.seed()`, and start a set of parallel workers with the `parallel::makeCluster()` and `doParallel::registerDoParallel()` commands.

The "learning" computation can take tens of minutes, or hours, or even days depending on the number of variates and data in your inference problem. The `learn()` function outputs various messages on how the computation is proceeding. As an example:

```{r eval = FALSE}
learnt <- learn(
    data = datafile,
    metadata = metadatafile,
    outputdir = 'parkinson_computations',
    seed = 16,
    parallel = 12)

#> Registered doParallelSNOW with 12 workers
#>
#> Using 30 datapoints
#> Calculating auxiliary metadata
#>
#> ************************** 
#> Saving output in directory
#> parkinson_computations
#> ************************** 
#> Starting Monte Carlo sampling of 3600 samples by 60 chains
#> in a space of 703 (effectively 6657) dimensions.
#> Using 12 cores: 60 samples per chain, 5 chains per core.
#> Core logs are being saved in individual files.
#>
#> C-compiling samplers appropriate to the variates (package Nimble)
#> this can take tens of minutes with many data or variates.
#> Please wait...
```

At the end of the computation the `learn()` function returns the name of the directory in which the results are saved. In the code snipped above, this name is saved in the variable `learnt`.


*(TO BE CONTINUED)*
